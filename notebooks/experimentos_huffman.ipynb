{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhEwCrLzVSQC"
   },
   "source": [
    "# **Codificación Huffman**\n",
    "Este cuaderno interactivo presenta una exploración completa del **algoritmo de codificación de Huffman**, desde su implementación fundamental hasta su análisis de rendimiento en diversos escenarios. El objetivo es demostrar la eficacia y versatilidad de esta técnica de compresión sin pérdida.\n",
    "\n",
    "El contenido está estructurado en tres secciones principales:\n",
    "\n",
    "1.  **El Motor de Compresión:** En esta primera parte, se definen todas las funciones esenciales que componen nuestro compresor. Aquí se encuentra la lógica para analizar archivos, aplicar el algoritmo de Huffman, calcular métricas de compresión y generar las visualizaciones para el análisis.\n",
    "\n",
    "2.  **Aplicaciones Prácticas:** Se demuestra el uso del algoritmo en casos concretos. Exploramos cómo se comporta al comprimir diferentes tipos de archivos, como un **texto literario** extraído del Quijote y un archivo de **código fuente** en Python, destacando las diferencias en sus resultados.\n",
    "\n",
    "3.  **Experimentos y Análisis de Rendimiento:** Finalmente, se somete el algoritmo a una serie de pruebas para evaluar su eficiencia. Se analiza su rendimiento en distintos escenarios (datos normales, repetitivos y aleatorios) y se estudia su **escalabilidad** al enfrentarse a archivos de tamaños crecientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías\n",
    "En esta celda se importan las librerías que se usan en el notebook.\n",
    "Además se define la variable *project_root* en la cual se almacena el directorio raíz del proyecto, ya que este notebook está bajo el directorio *notebooks/* lo cual le limita el acceso a los demás archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTMwXdorKT6x",
    "outputId": "8651aad1-1ffa-47ee-9274-22f144cbb792",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import string \n",
    "# Aqui se añade la carpeta root del proyecto, ya que se esta trabajando en notebook/\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones del encoder\n",
    "### Función Principal de Compresión y Análisis\n",
    "Esta función es el núcleo del motor. Recibe la ruta de un archivo, lo comprime y devuelve un diccionario con las estadísticas del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder.compressor_huffman import CompresorHuffman\n",
    "\n",
    "def analizar_compresion(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Toma la ruta de un archivo, lo comprime con Huffman y devuelve\n",
    "    un diccionario con las estadísticas del proceso.\n",
    "    \"\"\"\n",
    "    compresor = CompresorHuffman()\n",
    "    \n",
    "    # Manejo de excepción, por si no encuentra el archivo\n",
    "    try:\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "            texto = f.read()\n",
    "        # Se obtiene el tamaño del archivo antes de la compresión\n",
    "        tamano_original_bytes = os.path.getsize(ruta_archivo)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo {ruta_archivo} no fue encontrado.\")\n",
    "        return None\n",
    "\n",
    "    # Se realiza la compresión\n",
    "    texto_comprimido, raiz = compresor.compresion_huffman(texto)\n",
    "\n",
    "    # Aqui se calcula el tamaño de los archivos para el posterior analisis\n",
    "    longitud_en_bits = len(texto_comprimido)\n",
    "    tamano_datos_comprimidos_bytes = (longitud_en_bits + 7) // 8\n",
    "    \n",
    "    arbol_serializado = pickle.dumps(raiz)\n",
    "    tamano_arbol_bytes = len(arbol_serializado)\n",
    "    \n",
    "    tamano_total_comprimido_bytes = tamano_datos_comprimidos_bytes + tamano_arbol_bytes\n",
    "    \n",
    "    # Se calcula el ratio  = tamaño despues / tamaño antes\n",
    "    if tamano_original_bytes > 0:\n",
    "        ratio_compresion = (1 - (tamano_total_comprimido_bytes / tamano_original_bytes)) * 100\n",
    "    else:\n",
    "        ratio_compresion = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Se devuelven los resultados para la comprensi[on en forma de diccionario.\n",
    "    Esto para su posterior analisis\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"archivo\": os.path.basename(ruta_archivo),\n",
    "        \"original_b\": tamano_original_bytes,\n",
    "        \"total_comprimido_b\": tamano_total_comprimido_bytes,\n",
    "        \"ratio_%\": ratio_compresion\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para Procesar Lotes de Archivos\n",
    "Esta utilidad toma una lista de archivos, los procesa usando la función anterior y muestra un resumen comparativo en formato de tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def obtener_resultados(archivos_a_probar):\n",
    "    \"\"\"\n",
    "    aqui se definen los archivos y se obtienen los resultados.\n",
    "    \"\"\"\n",
    "    # Lista de archivos para los exp\n",
    "\n",
    "    lista_de_resultados = []\n",
    "    \n",
    "    for archivo in archivos_a_probar:\n",
    "        resultado = analizar_compresion(archivo)\n",
    "        if resultado:\n",
    "            lista_de_resultados.append(resultado)\n",
    "\n",
    "\n",
    "    # Resumen\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Resumen de Resultados de Compresión Huffman\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Archivo':<25} | {'Original (B)':>12} | {'Comprimido (B)':>15} | {'Ratio (%)':>10}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for res in lista_de_resultados:\n",
    "        print(f\"{res['archivo']:<25} | {res['original_b']:>12} | {res['total_comprimido_b']:>15} | {res['ratio_%']:>9.2f}%\")\n",
    "        \n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return lista_de_resultados\n",
    "    \n",
    "# Ejemplo del formato que debe seguir la lista de archivos a probar:\n",
    "archivos_a_probar = [\n",
    "        '../data/quijote.txt',\n",
    "        '../data/texto_repetitivo.txt',\n",
    "        '../data/texto_aleatorio.txt'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para analisis de datos\n",
    "### Utilidad: Gráfico Comparativo de Tamaños\n",
    "Esta función genera un gráfico de barras que compara el tamaño original contra el tamaño comprimido de los archivos procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_grafico_comparativo(resultados):\n",
    "    \"\"\"\n",
    "    Crea un gráfico de barras agrupadas comparando el tamaño\n",
    "    original vs el comprimido para cada archivo.\n",
    "    \"\"\"\n",
    "    # Extraer los datos para graficar\n",
    "    nombres_archivos = [res['archivo'] for res in resultados]\n",
    "    tamanos_originales = [res['original_b'] for res in resultados]\n",
    "    tamanos_comprimidos = [res['total_comprimido_b'] for res in resultados]\n",
    "    \n",
    "    # Configuración del gráfico\n",
    "    x = np.arange(len(nombres_archivos))  # las etiquetas ubicaciones\n",
    "    ancho_barra = 0.35  # ancho de barras\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # crear las barras\n",
    "    rects1 = ax.bar(x - ancho_barra/2, tamanos_originales, ancho_barra, label='Original', color='#4c72b0')\n",
    "    rects2 = ax.bar(x + ancho_barra/2, tamanos_comprimidos, ancho_barra, label='Comprimido', color='#c44e52')\n",
    "\n",
    "    # Aanadir títulos y etiquetas\n",
    "    ax.set_title('Comparación de Tamaño: Original vs. Comprimido', fontsize=16, weight='bold')\n",
    "    ax.set_ylabel('Tamaño (bytes)', fontsize=12)\n",
    "    ax.set_xlabel('Archivos de Prueba', fontsize=12)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(nombres_archivos, rotation=15, ha=\"right\")\n",
    "    ax.legend()\n",
    "\n",
    "    # añadir etiquetas de valor sobre las barras para mayor claridad\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "    ax.bar_label(rects2, padding=3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('results')\n",
    "    plt.savefig('../results/grafico_comparativo_tamanos.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilidad: Generador de Datos para Pruebas de Escalabilidad\n",
    "Esta función crea archivos de texto de diferentes tamaños para poder analizar cómo escala el rendimiento del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_datos_escalabilidad(tipo='normal', tamaños_kb=[1, 5, 10, 20, 50]):\n",
    "    \"\"\"\n",
    "    Genera archivos de prueba de diferentes tamaños para un tipo de texto específico.\n",
    "    Tipos: 'normal', 'repetitivo', 'aleatorio'\n",
    "    \"\"\"\n",
    "    # Cargar el texto base\n",
    "    if tipo == 'normal':\n",
    "        with open('../data/quijote.txt', 'r', encoding='utf-8') as f:\n",
    "            texto_base = f.read()\n",
    "    elif tipo == 'repetitivo':\n",
    "        texto_base = \"abcde\" * 20000 # ~100KB de texto repetitivo\n",
    "    else: # aleatorio\n",
    "        texto_base = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(100 * 1024))\n",
    "\n",
    "    # Crear sub-archivos de diferentes tamaños\n",
    "    if not os.path.exists('../data/temp'):\n",
    "        os.makedirs('../data/temp')\n",
    "\n",
    "    for kb in tamaños_kb:\n",
    "        longitud = kb * 1024\n",
    "        contenido = (texto_base * (longitud // len(texto_base) + 1))[:longitud]\n",
    "        with open(f'../data/temp/{tipo}_{kb}kb.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilidad: Gráfico de Análisis de Escalabilidad\n",
    "Finalmente, esta función visualiza los resultados de las pruebas de escalabilidad, mostrando la relación entre el tamaño original y el comprimido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_escalabilidad(df_resultados):\n",
    "    \"\"\"\n",
    "    Crea un gráfico de líneas mostrando cómo escala el tamaño comprimido\n",
    "    respecto al tamaño original para diferentes tipos de archivos.\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Línea base de \"sin compresión\" (y=x)\n",
    "    max_size = df_resultados['original_b'].max()\n",
    "    ax.plot([0, max_size], [0, max_size], 'k--', label='Sin Compresión (y=x)')\n",
    "\n",
    "    # Un color para cada tipo de archivo\n",
    "    colores = {'normal': '#4c72b0', 'repetitivo': '#55a868', 'aleatorio': '#c44e52'}\n",
    "\n",
    "    # Agrupar por tipo y graficar una línea para cada uno\n",
    "    for tipo, grupo in df_resultados.groupby('Tipo'):\n",
    "        ax.plot(grupo['original_b'], grupo['total_comprimido_b'], marker='o', linestyle='-', label=f'Texto {tipo}', color=colores[tipo])\n",
    "\n",
    "    # Títulos y etiquetas\n",
    "    ax.set_title('Análisis de Escalabilidad de la Compresión Huffman', fontsize=16, weight='bold')\n",
    "    ax.set_xlabel('Tamaño Original (bytes)', fontsize=12)\n",
    "    ax.set_ylabel('Tamaño Comprimido (bytes)', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/grafico_escalabilidad.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicaciones Prácticas\n",
    "\n",
    "## Aplicación 1: Compresor de Archivos de Texto (.txt)\n",
    "\n",
    "La aplicación más clásica y fundamental de la codificación de Huffman es la **compresión de archivos de texto**. En idiomas como el español o el inglés, la frecuencia de los caracteres no es uniforme: algunas letras como la 'e' o la 'a' y el carácter de espacio son extremadamente comunes, mientras que otras como la 'k', 'w' o 'x' son muy raras.\n",
    "\n",
    "El algoritmo de Huffman explota esta característica asignando **códigos binarios más cortos a los caracteres más frecuentes** y códigos más largos a los menos frecuentes. Esto resulta en una representación del texto que ocupa significativamente menos espacio que el formato estándar (como ASCII o UTF-8), donde cada carácter tiene una longitud fija (generalmente 8 bits).\n",
    "\n",
    "A continuación, aplicaremos nuestro compresor a un extracto del libro \"Don Quijote de la Mancha\" para demostrar su eficacia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos la ruta del archivo de texto que vamos a comprimir.\n",
    "# Usamos el mismo que en el experimento para mantener la consistencia.\n",
    "ruta_quijote = '../data/quijote.txt'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"APLICACIÓN 1: ANÁLISIS DE COMPRESIÓN DE TEXTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Utilizamos la función principal para analizar la compresión del archivo.\n",
    "resultado_texto = analizar_compresion(ruta_quijote)\n",
    "\n",
    "if resultado_texto:\n",
    "    # Imprimimos los resultados de una manera clara y descriptiva.\n",
    "    print(f\"Archivo Analizado:   {resultado_texto['archivo']}\")\n",
    "    print(f\"Tamaño Original:      {resultado_texto['original_b']} bytes\")\n",
    "    print(f\"Tamaño Comprimido:    {resultado_texto['total_comprimido_b']} bytes\")\n",
    "    print(f\"Ratio de Compresión:  {resultado_texto['ratio_%']:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"No se pudo procesar el archivo. Revisa que la ruta sea correcta.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación 2: Compresor de Código Fuente (.py)\n",
    "\n",
    "Una aplicación interesante y distinta es la compresión de **código fuente**. Aunque el código también es texto, su distribución de caracteres es muy diferente a la del lenguaje natural.\n",
    "\n",
    "En el código fuente (por ejemplo, de Python) son muy frecuentes caracteres como los **espacios de indentación, paréntesis `()`, puntos `.`, comas `,` y saltos de línea**. En cambio, la variedad de letras del alfabeto puede ser menor o tener una distribución diferente.\n",
    "\n",
    "Esta estructura particular hace que el código fuente sea un buen candidato para la compresión con Huffman. A continuación, comprimiremos el propio motor de compresión de este proyecto para evaluar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aplicación 2: Compresión de un archivo de código fuente ---\n",
    "\n",
    "# Definimos la ruta del archivo de código.\n",
    "ruta_codigo_fuente = '../data/config.py'\n",
    "# El archivo .py que se utilizó fue sacado de el repositorio oficial de Django\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"APLICACIÓN 2: ANÁLISIS DE COMPRESIÓN DE CÓDIGO FUENTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analizamos la compresión del archivo de código.\n",
    "resultado_codigo = analizar_compresion(ruta_codigo_fuente)\n",
    "\n",
    "if resultado_codigo:\n",
    "    # Imprimimos los resultados.\n",
    "    print(f\"Archivo Analizado:   {resultado_codigo['archivo']}\")\n",
    "    print(f\"Tamaño Original:      {resultado_codigo['original_b']} bytes\")\n",
    "    print(f\"Tamaño Comprimido:    {resultado_codigo['total_comprimido_b']} bytes\")\n",
    "    print(f\"Ratio de Compresión:  {resultado_codigo['ratio_%']:.2f}%\")\n",
    "else:\n",
    "    print(\"No se pudo procesar el archivo. Revisa que la ruta sea correcta.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación 3: Compresión de Secuencias de ADN (.fasta, .txt)\n",
    "\n",
    "Una de las aplicaciones más impactantes de la compresión de datos se encuentra en la **bioinformática**, específicamente en el almacenamiento de secuencias genómicas. El ADN se representa con un alfabeto increíblemente pequeño, compuesto casi en su totalidad por cuatro bases nitrogenadas: **Adenina (A), Citosina (C), Guanina (G) y Timina (T)**.\n",
    "\n",
    "Este escenario es ideal para la codificación de Huffman. Al tener un conjunto tan limitado y repetitivo de caracteres, el algoritmo puede asignar códigos binarios extremadamente cortos a estas cuatro letras, logrando **ratios de compresión muy elevados**.\n",
    "\n",
    "En un campo donde se generan terabytes de datos genómicos, la capacidad de comprimir eficientemente esta información es fundamental para el almacenamiento y la transmisión de datos en bases de datos a nivel mundial. A continuación, se demuestra esta capacidad comprimiendo un archivo que simula una secuencia de ADN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "APLICACIÓN 3: ANÁLISIS DE COMPRESIÓN DE SECUENCIA DE ADN\n",
      "============================================================\n",
      "Archivo Analizado:   adn.txt\n",
      "Tamaño Original:      1000000 bytes\n",
      "Tamaño Comprimido:    250265 bytes\n",
      "Ratio de Compresión:  74.97%\n",
      "\n",
      "Conclusión: Como se esperaba, el alfabeto reducido del ADN permite\n",
      "un ratio de compresión excepcionalmente alto, demostrando la gran\n",
      "utilidad de Huffman en el campo de la bioinformática.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Este archivo contiene una secuencia larga de los caracteres A, C, G, T.\n",
    "ruta_adn = '../data/adn.txt'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"APLICACIÓN 3: ANÁLISIS DE COMPRESIÓN DE SECUENCIA DE ADN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Utilizamos la misma función de análisis de siempre.\n",
    "resultado_adn = analizar_compresion(ruta_adn)\n",
    "\n",
    "if resultado_adn:\n",
    "    # Imprimimos los resultados.\n",
    "    print(f\"Archivo Analizado:   {resultado_adn['archivo']}\")\n",
    "    print(f\"Tamaño Original:      {resultado_adn['original_b']} bytes\")\n",
    "    print(f\"Tamaño Comprimido:    {resultado_adn['total_comprimido_b']} bytes\")\n",
    "    print(f\"Ratio de Compresión:  {resultado_adn['ratio_%']:.2f}%\")\n",
    "    print(\"\\nConclusión: Como se esperaba, el alfabeto reducido del ADN permite\")\n",
    "else:\n",
    "    print(\"No se pudo procesar el archivo. Asegúrate de que el archivo 'adn.txt' exista en la carpeta /data.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos\n",
    "## Experimento 1: Pruebas en 3 escenarios\n",
    "Se pone a prueba el algoritmo de codificación de mensaje en tres casos: un texto promedio (el inicio del libro \"Don Quijote de la mancha\"), un texto con los mismos 10 caracteres repetidos y por último un texto con caracteres aleatorios.\n",
    "En los tres casos se busco igualar la cantidad de caracteres a aproximadamente 10351, que es una cifra dispuesta de manera arbitraria. De manera que todos tienen aproximadamente la misma cantidad de caracteres y en lo que difieren es en la frecuencia y el rango o variedad de estos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos_a_probar = [\n",
    "        '../data/quijote.txt',\n",
    "        '../data/texto_repetitivo.txt',\n",
    "        '../data/texto_aleatorio.txt'\n",
    "    ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_exp1 = obtener_resultados(archivos_a_probar)\n",
    "    if results_exp1:\n",
    "        crear_grafico_comparativo(results_exp1)\n",
    "        print(\"Gráfico guardado en la carpeta /results/\")\n",
    "    else:\n",
    "        print(\"No hay resultados que mostrar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experimento 2: Escalabilidad del Algoritmo\n",
    "\n",
    "En este segundo experimento, el objetivo es analizar la **escalabilidad y la eficiencia** del algoritmo de Huffman a medida que aumenta el tamaño de los datos de entrada. Se busca observar cómo se comporta el ratio de compresión cuando los archivos a procesar son progresivamente más grandes.\n",
    "\n",
    "Para ello, se generan archivos de prueba de distintos tamaños (desde 1 KB hasta 100 KB) para cada uno de los tres tipos de texto: normal (basado en el Quijote), altamente repetitivo y aleatorio. Posteriormente, se grafica el tamaño original contra el tamaño comprimido para cada categoría. Esto nos permite visualizar de manera clara cómo escala la compresión y si la naturaleza de los datos influye en su rendimiento a mayor escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_escalabilidad():\n",
    "    tipos_de_texto = ['normal', 'repetitivo', 'aleatorio']\n",
    "    tamaños_kb = [1, 5, 10, 25, 50, 100] # Tamaños en Kilobytes\n",
    "    \n",
    "\n",
    "    for tipo in tipos_de_texto:\n",
    "        preparar_datos_escalabilidad(tipo, tamaños_kb)\n",
    "\n",
    "\n",
    "    # Lista para guardar todos los resultados\n",
    "    resultados_totales = []\n",
    "    \n",
    "    for tipo in tipos_de_texto:\n",
    "        for kb in tamaños_kb:\n",
    "            ruta = f'../data/temp/{tipo}_{kb}kb.txt'\n",
    "            resultado = analizar_compresion(ruta)\n",
    "            if resultado:\n",
    "                resultado['Tipo'] = tipo # Añadir el tipo para agrupar después\n",
    "                resultados_totales.append(resultado)\n",
    "    \n",
    "    if not resultados_totales:\n",
    "        print(\"No se generaron resultados. Verifica las rutas de los archivos.\")\n",
    "        return\n",
    "\n",
    "    # Convertir a df \n",
    "    df_final = pd.DataFrame(resultados_totales)\n",
    "    graficar_escalabilidad(df_final)\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutar el nuevo experimento\n",
    "visualizar_escalabilidad()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
